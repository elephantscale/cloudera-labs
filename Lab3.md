# Lab 3 - Hadoop Streaming MapReduce

In this lab, we will learn how to create Hadoop Streaming MapReduce jobs. Hadoop Streaming MapReduce API allows languages other than Java (e.g. Perl, Ruby, Python, etc.) to be used in processing files stored on the HDFS.

Streaming API is geared towards text processing and this processing model promotes Test-Driven development where the map and reduce modules can be tested in commandline in isolation of each other before hooking them into the Hadoop MapReduce framework.

## Part 1 - Connect to the Lab Environment

Note: Skip this Lab ## Part if you are using the Lab Server's desktop environment setup.

Using your SSH client, connect to the Lab Environment. Use cloudera/cloudera credentials when prompted.

## Part 2 - Getting the Input and Source Files

First, let's change to the Works working directory.

1. In the terminal window type in the following command:

```bash
cd ~/Works
```

The lab files are stored in the /home/cloudera/LabFiles/Streaming directory that we need

to copy over to the Streaming folder in the Works folder.

2. In the terminal window, run the following command:

```bash
cp -r /home/cloudera/LabFiles/Streaming/ .
```

This command will copy the Streaming folder with all its content over to the Works folder.

3. Enter the following command to change directory to /home/cloudera/Works/Streaming:

```bash
cd Streaming
```

The /home/cloudera/Works/Streaming directory is going to be our working directory for this lab.

The BASKERVILLES.txt file contains the The Hound of the Baskervilles novel by Arthur Conan Doyle that we will be using as the input file in this lab.



## Part 3 - Using Linux Tools to Imitate the Streaming MapReduce

Before we proceed with Hadoop's Streaming MapReduce API steps, let's see how we can perform simple "map & reduce" operations using standard Linux tools. For example, let's see how we can count the number of characters, words and lines in the BASKERVILLES.txt file.

We will use the cat and wc utilities for that.

The cat (concatenate) utility prints contents of file(s) to standard output and the wc (word count) utility prints the number of lines, words and characters in the file passed as an argument or on standard input.

You can get more help on these tools by typing cat --help and wc --help 1. Enter the following command:

```bash
cat BASKERVILLES.txt | wc
```

You should see the following output:

```console
7573
59618
323611
```

So we have 7573 lines, 59618 words and 323611 characters (bytes) in the input file. Note: We piped the output generated by the cat tool via the pipe (|) operator to wc which did the counting of elements in the input file.

Suppose, you only want to count the number of the words. The wc tool offers this possibility.

2. Enter the following command:

```bash
cat BASKERVILLES.txt | wc -w
```

The output should be 59618 which corresponds to the number of words in the previous command example.

The file is not big, only 317K, and it nicely fits in our machine RAM and can be efficiently processed locally. The story will change if we need to process 317 TB of data for calculating the same metrics (lines, words, and characters).

So, let's see how we can use Hadoop Streaming for the same task.


## Part 4 - Import the Input File into HDFS

For MapReduce jobs, it is advisable to create a separate folder in HDFS to hold the input file(s).

Let's create the required directory and name it IN (it can be named differently as long as it is a HDFS-compliant name). The output directory that contains the processing results will be created by the MapReduce job itself (but we configure it to be OUT).

First, let's make sure there is no IN folder which may have been left over from any of the previous labs.

1. Enter the following command to delete the IN folder (if any exists in HDFS) with any contents it may have:

```bash
hadoop fs -rm -r -skipTrash IN
```

2. Enter the following command to delete the OUT folder (if any exists in HDFS):

```bash
hadoop fs -rm -r -skipTrash OUT
```

3. Enter the following command to create the IN directory from scratch:

```bash
hadoop fs -mkdir IN
```

4. Run the following two command:

```bash
hadoop fs -put BASKERVILLES.txt IN/
```

If you want to, you can verify that the file has been, indeed, copied correctly. You can do this by using the following HDFS commands:

```bash
hadoop fs -ls IN/
```

This command will show the directory listing of the IN folder on HDFS:

```console
Found 1 items -rw-r--r-- 1 cloudera cloudera IN/BASKERVILLES.txt

323611 2015-06-25 07:17
```



## Part 5 - Hadoop Streaming MapReduce

Let's see how to use the Hadoop's Streaming MapReduce API. The actual commands for running our Streaming MapReduce job have been included in the streamingScript.sh. 1. Enter the following command (to open the script in read-only mode):

```bash
vi -R streamingScript.sh
```

You should see the following output:

```bash

#---------------------------
# Using Hadoop streaming API
# --------------------------

export SLIB=/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.4.0.jar

hadoop jar $SLIB  \
    -input    IN  \
    -output   OUT \
    -mapper   /bin/cat \
    -reducer  "/usr/bin/wc -w"
```

The input / output streaming management and integration with the MapReduce engine is performed by the hadoop-streaming-2.6.0-cdh5.4.0.jar Java library file (its version number depends on Hadoop version and the distribution package).

Hadoop takes this jar file as its argument along with the input (IN) and output (OUT) folders, as well as the mapper and reducer modules that support the Standard Input and Output piping.

2. Close the editor by pressing Esc followed by : followed by q (at the : command prompt).

Before we can run this script, we, first, need to setup proper executable permissions for this file.

3. Enter the following command:

```bash
chmod u+x streamingScript.sh
```

Now everything is ready, so let's roll. 4. Enter the following command:

```bash
./streamingScript.sh
```



21 You should see the progress of the MapReduce job triggered by our script. Upon its completion (be extra patient!), you should see the following output in your console (shown below abridged for space); your job id will be different.

```console

15/06/25 07:38:43 INFO mapreduce.Job: Job job_1435237446596_0001 running in uber mode : false 15/06/25 07:38:43 INFO mapreduce.Job: map 0% reduce 0% 15/06/25 07:41:17 INFO mapreduce.Job: map 100% reduce 0% 15/06/25 07:42:06 INFO mapreduce.Job: map 100% reduce 100% 15/06/25 07:42:06 INFO mapreduce.Job: Job job_1435237446596_0001 completed successfully â€¦

15/06/25 07:42:06 INFO streaming.StreamJob: Output directory: OUT

You can review the status of the job by opening your web browser and going to the URL printed out to your console right before your job was accepted for execution; in our case, the URL was specified in this line:

.. INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1435237446596_0001/
```

Here is the job's overview page when the job has been completed.

A great deal of information and insights can be gleaned from the Configuration page which combines configuration parameters from Hadoop configuration files (as well default configuration properties not listed in configuration files) used when running your job.

Now let's verify the job's results saved on HDFS.



5. Enter the following command:

```bash
hadoop fs -cat OUT/part-00000
```

You should see the number of words in the input file.

59618 Which matches our previous results.

Note: The part-00000 file is generated by the MapReduce engine and populated with the job's processing results.

We will not need this data anymore.

6. Enter the following command as a clean-up:

```bash
hadoop fs -rm -r -skipTrash OUT
```

Next, we are going to demonstrate how to use other scripting languages in the Streaming MapReduce framework. We will do this with Python-based mapper and reducer.

## Part 6 - Reviewing Python-based Mapper and Reducer

1. Enter the following command to review the mapper:

```bash
vi -R mapper.py
```

Here is thec ontents of teh file [mapper.py](./LabFiles/Streaming/mapper.py)

```python
#!/usr/bin/env python
import sys

emitCount = 1

# Get lines from standard input
for line in sys.stdin:
    lineOfWords = line.strip().split()
    for word in lineOfWords:
        print '%s\t%s' % (word,emitCount)
        
# To test:
# cat file.dat | ./mapper.py
# echo "list of words" | ./mapper.py
```

Review the contents of the file. Python has clear syntax so even if you are not familiar with the language, you should be able to follow the coded logic.



The program splits every line received from the standard input into tokens (words) and sends each one of them with the count of 1 to the standard output (the print() statement). The separator between the word (key) and its count is the tab character.

One of the advantages of the Streaming MapReduce is that you can test mappers and reducers in isolation.

We do it only with the mapper.py.

But first we need to make our Python scripts executable.

2. Exit vi The vi window should close and you be back at the command prompt.

3. Enter the following command:

```bash
chmod 0777 *.py
```

This command will set the necessary executable permissions on our Python files. 4. Enter the following command:

```bash
cat BASKERVILLES.txt | ./mapper.py
````

You should see the output of our mapper.py which emits each word in the input file with the count of one.

Now let's review the reducer.py file.

5. Enter the following command:

```bash
vi -R reducer.py
```

You should see the source of [reducer.py](./LabFiles/Streaming/reducer.py).

```python
#!/usr/bin/env python
import sys

current_key = None  # null
current_key_count = 0

# Get lines from standard input
for line in sys.stdin:
	key,count= line.strip().split('\t')
	count = int(count)
	
	if current_key == key:
		current_key_count += count
	else:
		if current_key is not None:
			print "%s has %s occurrence(s)" % (current_key, current_key_count)
		current_key = key # first assignment
		current_key_count = count # usually, start with 1

# The last key summary ...
print "%s has %s occurrence(s)" % (current_key, current_key_count)
```

This code is a bit more complex than mappter.py, but the logic is also straightforward: we count the number of occurrences of each key (word). Every line that comes through the standard input corresponds to the output of the mapper.py generated by its print '%s\t%s' % (word,emitCount) statement.

The program expects the input to come in an orderly fashion. For it to function properly, the input must be sorted by key before it is send to it via standard input. And that's exactly what the shuffle step does in MapReduce.

6. Exit vi

Let's test our mapper and reducer on the Linux command-line.

7. Enter the following command:

```bash
cat BASKERVILLES.txt | ./mapper.py | sort | ./reducer.py
```

You should see the following output (below shown abridged for space)

...

your has 164 occurrence(s) "Your has 4 occurrence(s) Your has 8 occurrence(s) you're has 1 occurrence(s) "You're has 1 occurrence(s) yours, has 3 occurrence(s) yours. has 2 occurrence(s) yours has 2 occurrence(s) yourself," has 1 occurrence(s) yourself, has 1 occurrence(s) yourself?" has 2 occurrence(s) yourself." has 3 occurrence(s) yourself. has 3 occurrence(s) yourself has 9 occurrence(s) ...

This output is generated by the

```python
print "%s has %s occurrence(s)" % (current_key, current_key_count)
```

line in reducer.py.



25 The sort tool that is piped in between the mapper and reducer imitates the shuffle phase and also helps with the reducer logic.

Let's now run the actual Streaming MapReduce job using our mapper.py and reducer.py.

## Part 7 - Running the Streaming MapReduce Job

The driver script is called [streamingPython.sh](./LabFiles/Streaming/streamingPython.sh). 

1. Enter the following command:

```bash
vi -R streamingPython.sh
```

You should see the following output which is similar to that of the streamingScript.sh script we reviewed before:

```bash

#---------------------------
# Using Hadoop streaming API
# --------------------------

export SLIB=/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.4.0.jar

hadoop jar $SLIB  \
    -input    IN  \
    -output   OUT \
    -mapper   "/home/cloudera/Works/Streaming/mapper.py" \
    -reducer  "/home/cloudera/Works/Streaming/reducer.py"
```

2. Exit vi Before we can run this script, we need to make it executable. 3. Enter the following command:

```bash
chmod u+x streamingPython.sh
```

We are now ready to run the job. But before that, just in case you were experimenting outside of the regular lab flow, let's do some house cleaning.

4. Enter the following command (this is to make sure the output folder is gone):

```bash
hadoop fs -rm -r -skipTrash OUT
```

Now we can run our Streaming MapReduce job with the Python-based mapper and reducer.



5. Enter the following command:

```bash
./streamingPython.sh
```

When the job is done, you can review the contents of the OUT directory.

6. Enter the following command:

```bash
hadoop fs -cat OUT/part-00000
```

You should see the output that is similar to the one we received in our Linux command line. Sorting done by the shuffle step is a bit different from that done by the sort tool but the actual word counts are the same.

Take a moment to analyze the output on your console.

## Part 8 - Improving Your MapReduce Application [Optional]

The astute reader will notice that our application is not particularly accurate when counting the words as it treats them as different words when they come together with punctuation marks, e.g.

```console
yours;
yours. 
yours,
```

will be treated as three distinct tokens even though they have the same base ("yours").

In this Lab part, we are going to fix this deficiency.

1. Enter the following command:

```bash
cd /home/cloudera/Works/Streaming/
```

2. Enter the following command (in one line):

```bash
cat BASKERVILLES.txt | ./mapper.py | sort | ./reducer.py | grep -i yourself
```

You should see the following output:

```console
yourself," has 1 occurrence(s) 
yourself, has 1 occurrence(s) 
yourself?" has 2 occurrence(s) 
yourself." has 3 occurrence(s)
yourself. has 3 occurrence(s) 
yourself has 9 occurrence(s)
```


As you can see, we have got a number of variations of yourself that we are planning to start treating uniformly as the yourself token by filtering out the punctuation "noise". After the change we are going to make, we should be able to see the yourself has 19 occurrence(s) message instead.

But the question now is "Where should we place the punctuation filtering code: in the mapper or reducer module?" Take a moment to think about it.

We leave the rest of the page below intentionally blank so that you can proceed after you have tried to decide for yourself.

The code should be placed in the mapper module as doing so will make the Shuffle and Sort processing phase less busy as well as help conserve network bandwidth for data transfer to the Reduce phase. In our case, we need to modify the mapper.py file.

3. Enter the following command:

```bash
vi mapper.py
```

4. Add the following line under import sys (use no indentation):

```python
import re
```

This command will import the regular expressions module.

5. Enter the following line under emitCount = 1 (use no indentation):

```python
punctuations = '[",.?\'!;-]'
```

This is the list of punctuation marks we will strip off of the end of every word. 

```python
#!/usr/bin/env python
import sys

emitCount = 1
punctuations = '[",.?\'!;-]'

# Get lines from standard input
for line in sys.stdin:
    lineOfWords = line.strip().split()
    for word in lineOfWords:
        print '%s\t%s' % (word,emitCount)
        
# To test:
# cat file.dat | ./mapper.py
```

6. Enter the following three line (use 8 spaces for indentation) under for word in lineOfWords:


```python
word = word.lower() 
word = re.sub(punctuations, '', word) 
if (len(word) == 0): 
   continue
```

In the above code, we make every word in lowercase, strip off punctuation marks and get the next word from the system input if the stripping operation results in an empty word

(which we are not emitting).

The resulting code of mapper.py should look as follows (the added lines are shown in bold):

```python
#!/usr/bin/env python
import sys
import re

emitCount = 1
punctuations = '[",.?\'!;-]'

# Get lines from standard input
for line in sys.stdin:
    lineOfWords = line.strip().split()
    for word in lineOfWords:
        word = word.lower() 
        word = re.sub(punctuations, '', word) 
        if (len(word) == 0): 
            continue
        print '%s\t%s' % (word,emitCount)
        
# To test:
# cat file.dat | ./mapper.py
```

7. Save the mapper.py and exit vi 8. Repeat the command (in one line):

```bash
cat BASKERVILLES.txt | ./mapper.py | sort | ./reducer.py | grep -i yourself
```

You should see the following output:

```console
yourself has 19 occurrence(s)
```

That's exactly what we wanted to achieve.

## Part 9 - Working Area Clean-up

1. Enter the following command (don't forget /* at the end of the command!):

```bash
hadoop fs -rm -r -skipTrash /user/cloudera/*
```

You should see a confirmation message on the deleted resources.


2. Enter the following command:

```bash
cd /home/cloudera/Works
```

3. Enter the following command:

```bash
pwd
```

You should see the following output:

```bash
/home/cloudera/Works
```

4. Enter the following command:


```bash
rm -fr *
```

## Part 10 - Ending the Working Session

1. Type in exit and press Enter to close your terminal window.

This is the last step in this lab.

## Part 11 - Review

In this lab, we learned how to create and run Hadoop Streaming MapReduce jobs. We reviewed how to use the Linux command line for performing initial testing outside of the MapReduce framework.
