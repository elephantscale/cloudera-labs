from __future__ import print_function

import numpy as np
from pyspark import SparkContext
from pyspark.mllib.clustering import KMeans
from pyspark.mllib.clustering import KMeansModel

inputFile = "file:///home/cloudera/LabFiles/Spark/MLlib/kmeans_clusters.dat"
k =  3

sc = SparkContext(appName="KMeansWithMLlib")
sc.setLogLevel("ERROR") 
lines = sc.textFile(inputFile)

lines.foreach(print)

data = lines.map(lambda line: np.array([float(x) for x in line.split(' ')]))

model = KMeans.train(data, k, runs=10, epsilon=1e-3 )

for x in model.clusterCenters: print(x)

model.computeCost(data)

#path2Save = "file:///home/cloudera/Works/MyModel/"
#model.save(sc, path2Save)

#recoveredModel = KMeansModel.load(sc, path2Save)

sc.stop()



